#!/usr/bin/env bash

# QA Integration End-to-End Tests
# Tests the complete RAG → QA submission flow

# Find and source test helpers
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
TEST_HELPERS="$TETRA_SRC/bash/tmod/tests/test_helpers.sh"

if [[ ! -f "$TEST_HELPERS" ]]; then
    echo "Error: Test helpers not found at $TEST_HELPERS"
    exit 1
fi

source "$TEST_HELPERS"

# Test setup
setup_test_env() {
    # Create temporary test environment
    export TEST_TETRA_DIR=$(mktemp -d -t tetra-qa-test.XXXXXX)
    export TEST_RAG_DIR="$TEST_TETRA_DIR/rag"
    export TEST_QA_DIR="$TEST_TETRA_DIR/qa"

    mkdir -p "$TEST_RAG_DIR"/{db,config,logs}
    mkdir -p "$TEST_QA_DIR/db"

    # Override environment for testing
    export RAG_DIR="$TEST_RAG_DIR"
    export QA_DIR="$TEST_QA_DIR"

    echo "Test environment: $TEST_TETRA_DIR"
}

# Test cleanup
cleanup_test_env() {
    if [[ -n "$TEST_TETRA_DIR" ]] && [[ -d "$TEST_TETRA_DIR" ]]; then
        rm -rf "$TEST_TETRA_DIR"
        echo "Cleaned up: $TEST_TETRA_DIR"
    fi
}

# Mock qa_query function for testing without actual API calls
mock_qa_query() {
    # Read stdin and produce a mock answer
    local input=$(cat)

    # Simulate a simple answer based on the input
    cat <<EOF
# Mock QA Response

This is a mock answer to your question.

**Input received:** $(echo "$input" | head -n 1)

The QA system would normally process this through the OpenAI API,
but this is a test response.

---
*Generated by test mock at $(date)*
EOF
}

# Source RAG modules
source_rag_modules() {
    source "$RAG_SRC/core/flow_manager.sh"
    source "$RAG_SRC/core/evidence_manager.sh"
    source "$RAG_SRC/core/evidence_selector.sh"
    source "$RAG_SRC/core/assembler.sh"
    source "$RAG_SRC/core/qa_submit.sh"
}

# Test 1: Verify submit fails without active flow
test_submit_without_flow() {
    local output=$(submit_to_qa 2>&1)
    local exit_code=$?

    assert_failure $exit_code "Submit should fail without active flow"
    assert_contains "$output" "No active flow" "Should show 'No active flow' error"
}

# Test 2: Verify submit fails without QA module
test_submit_without_qa_module() {
    # Create a test flow
    local flow_dir="$TEST_RAG_DIR/db/test-flow-$(date +%s)"
    mkdir -p "$flow_dir/build"
    echo "Test prompt" > "$flow_dir/build/prompt.mdctx"

    # Make it the active flow
    echo "$flow_dir" > "$TEST_RAG_DIR/.active_flow"

    # Ensure qa_query is not available in subshell
    local output=$(unset -f qa_query 2>/dev/null; submit_to_qa 2>&1)
    local exit_code=$?

    assert_failure $exit_code "Submit should fail without QA module"
    assert_contains "$output" "QA module not loaded" "Should show QA module error"
}

# Test 3: Verify submit fails without assembled context
test_submit_without_context() {
    # Create a test flow without assembled context
    local flow_dir="$TEST_RAG_DIR/db/test-flow-$(date +%s)"
    mkdir -p "$flow_dir/build"

    # Make it the active flow
    echo "$flow_dir" > "$TEST_RAG_DIR/.active_flow"

    # Create mock qa_query in PATH for command -v to find
    local mock_bin="$TEST_TETRA_DIR/bin"
    mkdir -p "$mock_bin"
    cat > "$mock_bin/qa_query" <<'MOCK_SCRIPT'
#!/usr/bin/env bash
cat <<EOF
# Mock QA Response
Test response
EOF
MOCK_SCRIPT
    chmod +x "$mock_bin/qa_query"

    # Test with mock in PATH
    local output=$(PATH="$mock_bin:$PATH" submit_to_qa 2>&1)
    local exit_code=$?

    assert_failure $exit_code "Submit should fail without assembled context"
    assert_contains "$output" "Context not assembled" "Should show context error"
}

# Test 4: End-to-end flow test
test_e2e_qa_submission() {
    # Setup: Create flow with assembled context
    local flow_dir="$TEST_RAG_DIR/db/e2e-test-$(date +%s)"
    mkdir -p "$flow_dir/build"
    mkdir -p "$flow_dir/ctx/evidence"

    # Create test evidence
    echo "Sample code for testing" > "$flow_dir/ctx/evidence/sample.sh"

    # Create assembled prompt
    cat > "$flow_dir/build/prompt.mdctx" <<'EOF'
# Test Context

## Request
Please analyze this code.

## Evidence
```bash
# sample.sh
Sample code for testing
```
EOF

    # Create events file
    echo '{"ts":"'$(date -u '+%Y-%m-%dT%H:%M:%SZ')'","event":"flow_created"}' > "$flow_dir/events.ndjson"

    # Make it active
    echo "$flow_dir" > "$TEST_RAG_DIR/.active_flow"

    # Create mock qa_query script in PATH
    local mock_bin="$TEST_TETRA_DIR/bin"
    mkdir -p "$mock_bin"
    cat > "$mock_bin/qa_query" <<'MOCK_SCRIPT'
#!/usr/bin/env bash
cat <<EOF
# Mock QA Response

This is a mock answer to your question.

**Input received:** $(cat | head -n 1)

The QA system would normally process this through the OpenAI API,
but this is a test response.

---
*Generated by test mock*
EOF
MOCK_SCRIPT
    chmod +x "$mock_bin/qa_query"

    # Execute submission with mock in PATH
    local output=$(PATH="$mock_bin:$PATH" submit_to_qa 2>&1)
    local exit_code=$?

    # Verify success
    assert_success $exit_code "E2E submission should succeed"
    assert_contains "$output" "Submitting to QA" "Should show submission message"
    assert_contains "$output" "Answer saved" "Should show success message"

    # Verify answer file was created
    assert_true "[[ -f '$flow_dir/build/answer.md' ]]" "Answer file should exist"

    # Verify answer contains mock response
    local answer_content=$(cat "$flow_dir/build/answer.md")
    assert_contains "$answer_content" "Mock QA Response" "Answer should contain mock response"

    # Verify events logging
    local events_content=$(cat "$flow_dir/events.ndjson")
    assert_contains "$events_content" "qa_submit" "Events should contain qa_submit event"
}

# Test 5: Verify flow manager integration
test_flow_manager_integration() {
    # Create and activate flow using flow manager
    local flow_desc="Test QA integration"
    local flow_id=$(create_flow "$flow_desc" 2>/dev/null)

    if [[ -z "$flow_id" ]]; then
        echo "⚠ SKIP: Flow manager create_flow not available"
        return 0
    fi

    local flow_dir=$(get_active_flow_dir)

    assert_true "[[ -n '$flow_dir' ]]" "Active flow should be set"
    assert_true "[[ -d '$flow_dir' ]]" "Flow directory should exist"

    # Create minimal context
    mkdir -p "$flow_dir/build"
    echo "# Test" > "$flow_dir/build/prompt.mdctx"

    # Create mock qa_query script
    local mock_bin="$TEST_TETRA_DIR/bin"
    mkdir -p "$mock_bin"
    cat > "$mock_bin/qa_query" <<'MOCK_SCRIPT'
#!/usr/bin/env bash
echo "Mock answer"
MOCK_SCRIPT
    chmod +x "$mock_bin/qa_query"

    # Submit should work
    local output=$(PATH="$mock_bin:$PATH" submit_to_qa 2>&1)
    local exit_code=$?

    assert_success $exit_code "Should submit with flow manager integration"
}

# Test 6: Verify CLI routing (@qa validation)
test_cli_qa_routing() {
    # Test invalid target
    local output=$(bash -c 'source "$RAG_SRC/rag.sh"; rag submit @invalid 2>&1')
    local exit_code=$?

    assert_failure $exit_code "Should reject invalid target"
    assert_contains "$output" "Only @qa target supported" "Should show target error"
}

# Test 7: Verify answer file format
test_answer_file_format() {
    # Setup flow
    local flow_dir="$TEST_RAG_DIR/db/format-test-$(date +%s)"
    mkdir -p "$flow_dir/build"

    echo "# Test prompt" > "$flow_dir/build/prompt.mdctx"
    echo "$flow_dir" > "$TEST_RAG_DIR/.active_flow"

    # Create mock qa_query with specific output
    local mock_bin="$TEST_TETRA_DIR/bin"
    mkdir -p "$mock_bin"
    cat > "$mock_bin/qa_query" <<'MOCK_SCRIPT'
#!/usr/bin/env bash
echo "# Test Answer"
echo ""
echo "This is the answer content."
MOCK_SCRIPT
    chmod +x "$mock_bin/qa_query"

    # Submit
    PATH="$mock_bin:$PATH" submit_to_qa >/dev/null 2>&1

    # Check answer file exists and has content
    local answer_file="$flow_dir/build/answer.md"
    assert_true "[[ -f '$answer_file' ]]" "Answer file should be created"

    local answer=$(cat "$answer_file")
    assert_contains "$answer" "# Test Answer" "Answer should have header"
    assert_contains "$answer" "This is the answer content" "Answer should have content"
}

# Test 8: Verify event logging format
test_event_logging() {
    # Setup flow
    local flow_dir="$TEST_RAG_DIR/db/events-test-$(date +%s)"
    mkdir -p "$flow_dir/build"

    echo "# Test" > "$flow_dir/build/prompt.mdctx"
    echo '{}' > "$flow_dir/events.ndjson"
    echo "$flow_dir" > "$TEST_RAG_DIR/.active_flow"

    # Create mock qa_query
    local mock_bin="$TEST_TETRA_DIR/bin"
    mkdir -p "$mock_bin"
    cat > "$mock_bin/qa_query" <<'MOCK_SCRIPT'
#!/usr/bin/env bash
echo "Answer"
MOCK_SCRIPT
    chmod +x "$mock_bin/qa_query"

    # Submit
    PATH="$mock_bin:$PATH" submit_to_qa >/dev/null 2>&1

    # Check events file
    local events=$(tail -n 1 "$flow_dir/events.ndjson")
    assert_contains "$events" "qa_submit" "Event should have qa_submit type"
    assert_contains "$events" "@qa" "Event should reference @qa target"
    assert_contains "$events" "\"ts\":" "Event should have timestamp"
}

# Main test execution
run_all_tests() {
    echo "==================================="
    echo "QA Integration End-to-End Tests"
    echo "==================================="
    echo ""

    # Setup
    setup_test_env
    source_rag_modules

    # Run tests
    echo "--- Phase 1: Error Handling ---"
    test_submit_without_flow
    test_submit_without_qa_module
    test_submit_without_context

    echo ""
    echo "--- Phase 2: Happy Path ---"
    test_e2e_qa_submission
    test_flow_manager_integration

    echo ""
    echo "--- Phase 3: Validation ---"
    test_cli_qa_routing
    test_answer_file_format
    test_event_logging

    # Cleanup
    cleanup_test_env
}

# Execute all tests
run_all_tests
